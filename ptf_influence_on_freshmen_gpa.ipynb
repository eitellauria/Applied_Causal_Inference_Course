{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eitellauria/Applied_Causal_Inference_Course/blob/main/ptf_influence_on_freshmen_gpa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJNjAcGzFj01"
      },
      "source": [
        "# Causal Discovery example\n",
        "\n",
        "The goal of this notebook is to show how causal discovery methods can work with DoWhy. We use discovery methods from [causal-learn](https://github.com/py-why/causal-learn) repo. As we will see, causal discovery methods require appropriate assumptions for the correctness guarantees, adn thus there will be variance across results returned by different methods in practice. These methods, however, may be combined usefully with domain knowledge to construct the final causal graph."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dowhy --quiet\n",
        "!pip install econml --quiet"
      ],
      "metadata": {
        "id": "5oadZJFeFui2",
        "outputId": "105e924c-c529-4773-e263-46e9e40cdf13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.6/572.6 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymqTPEcgFj05"
      },
      "outputs": [],
      "source": [
        "import dowhy\n",
        "from dowhy import CausalModel\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import graphviz\n",
        "import networkx as nx\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EkUd3MGFj07"
      },
      "source": [
        "## Utility functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFYfaJ11Fj08"
      },
      "outputs": [],
      "source": [
        "#We define a utility function to draw the directed acyclic graph.\n",
        "def make_graph(adjacency_matrix, labels=None):\n",
        "    idx = np.abs(adjacency_matrix) > 0.01\n",
        "    dirs = np.where(idx)\n",
        "    d = graphviz.Digraph(engine='dot')\n",
        "    names = labels if labels else [f'x{i}' for i in range(len(adjacency_matrix))]\n",
        "    for name in names:\n",
        "        d.node(name)\n",
        "    for to, from_, coef in zip(dirs[0], dirs[1], adjacency_matrix[idx]):\n",
        "        d.edge(names[from_], names[to], label=str(coef))\n",
        "    return d\n",
        "\n",
        "def str_to_dot(string):\n",
        "    '''\n",
        "    Converts input string from graphviz library to valid DOT graph format.\n",
        "    '''\n",
        "    graph = string.strip().replace('\\n', ';').replace('\\t','')\n",
        "    graph = graph[:9] + graph[10:-2] + graph[-1] # Removing unnecessary characters from string\n",
        "    return graph\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLHk-2qiFj09"
      },
      "source": [
        "## 1. Load the data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XI8scdPAHrjd",
        "outputId": "56849a05-8708-4ae6-e634-a17257c6a8a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_source = pd.read_csv('/gdrive/MyDrive/Research/CausalInference/FT_PT vs GPA/student_course_data.csv')\n",
        "df_source.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zs10hDy4Hzoq",
        "outputId": "8d616548-c08d-4b5e-8b8f-a976ca34f1a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 38964 entries, 0 to 38963\n",
            "Data columns (total 27 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   SCHOOL              38964 non-null  object \n",
            " 1   TERM_CODE           38964 non-null  int64  \n",
            " 2   COURSE_SECTION      38964 non-null  object \n",
            " 3   FT                  38964 non-null  int64  \n",
            " 4   ENROLLMENT          38964 non-null  int64  \n",
            " 5   COURSE              38964 non-null  object \n",
            " 6   warehouseContactID  38964 non-null  int64  \n",
            " 7   AGE                 38956 non-null  float64\n",
            " 8   SEX                 38960 non-null  object \n",
            " 9   CLASS               38964 non-null  object \n",
            " 10  LAST_NAME           38964 non-null  object \n",
            " 11  FIRST_NAME          38964 non-null  object \n",
            " 12  MI                  29789 non-null  object \n",
            " 13  PELLSTATUS          38964 non-null  object \n",
            " 14  STUDENT_TYPE        38964 non-null  object \n",
            " 15  STUDENT_LEVEL       38964 non-null  object \n",
            " 16  MAJOR               38964 non-null  object \n",
            " 17  RACE_CODE           38964 non-null  object \n",
            " 18  RESIDENT_CODE       38964 non-null  object \n",
            " 19  ISPROBATION         38964 non-null  int64  \n",
            " 20  ISDEANSLIST         38964 non-null  int64  \n",
            " 21  GPA_U               21440 non-null  float64\n",
            " 22  STANDING_DESC       3557 non-null   object \n",
            " 23  HOURS_ATTEMPTED_U   36869 non-null  float64\n",
            " 24  HOURS_PASSED_G      8 non-null      float64\n",
            " 25  FIRST_SEMESTER      38964 non-null  int64  \n",
            " 26  GRADE_FINAL         36787 non-null  object \n",
            "dtypes: float64(4), int64(7), object(16)\n",
            "memory usage: 8.0+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data={'A':4,'A-':3.7,'B+':3.3,'B':3.0,'B-':2.7,'C+':2.3,'C':2.0,'C-':1.7,'D+':1.3,'D':1.0,'D-':0.7,'F':0}\n",
        "grades_lookup = pd.DataFrame(list(data.items()), columns=['LETTER_GRADE', 'NUMBER_GRADE'] )\n"
      ],
      "metadata": {
        "id": "T5msGhZRMJXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df1= df_source[['TERM_CODE','COURSE', 'FT','ENROLLMENT','AGE', 'SEX', 'CLASS', 'PELLSTATUS', 'RACE_CODE', 'FIRST_SEMESTER', 'GRADE_FINAL']]\n",
        "df1=df1.dropna().copy(deep=True)\n",
        "df1['COURSE_SECTION']=df1['COURSE']\n",
        "df1['FRESHMEN'] = np.where(df1['CLASS']==\"FR\", 1, 0)\n",
        "df1['RACE_MINORITY']=np.where(df1['RACE_CODE']=='W',0,1)\n",
        "df1['FEMALE']=np.where(df1['SEX']=='F',1,0)\n",
        "df1['PELLSTATUS']=np.where(df1['PELLSTATUS']=='YES',1,0)\n",
        "df1['POOR_PERFORMANCE'] = np.where(df1['GRADE_FINAL'].isin(['C-', 'D+', 'D', 'D-', 'F']), 1, 0)\n",
        "df1=df1.query('FIRST_SEMESTER==1')\n",
        "df1=df1[['TERM_CODE','COURSE_SECTION', 'FT','ENROLLMENT','AGE','FRESHMEN','RACE_MINORITY','FEMALE','PELLSTATUS','POOR_PERFORMANCE']]\n",
        "\n",
        "# Group the dataframe by TERM_CODE and COURSE_SECTION\n",
        "grouped_df = df1.groupby([\"TERM_CODE\", \"COURSE_SECTION\"])\n",
        "\n",
        "# Apply the aggregation functions to the grouped dataframe\n",
        "df2 = grouped_df.agg({\n",
        "    'FT': np.mean,\n",
        "    'ENROLLMENT':np.mean,\n",
        "    'AGE': np.mean,\n",
        "    'FEMALE': np.sum,\n",
        "    'FRESHMEN': np.sum,\n",
        "    'RACE_MINORITY': np.sum,\n",
        "    'FEMALE': np.sum,\n",
        "    'PELLSTATUS': np.sum,\n",
        "    'POOR_PERFORMANCE': np.sum\n",
        "}).reset_index()\n",
        "\n",
        "df2['FT'] = df2['FT'].astype('int32')\n",
        "df2['AVG_AGE'] = df2['AGE']\n",
        "df2['PCT_FRESHMEN'] = df2['FRESHMEN'] / df2['ENROLLMENT']*100\n",
        "df2['PCT_RACE_MINORITY'] = df2['RACE_MINORITY'] / df2['ENROLLMENT']*100\n",
        "df2['PCT_FEMALE'] = df2['FEMALE'] / df2['ENROLLMENT']*100\n",
        "df2['PCT_PELLSTATUS'] = df2['PELLSTATUS'] / df2['ENROLLMENT']*100\n",
        "df2['PCT_POOR_PERFORMANCE'] = df2['POOR_PERFORMANCE'] / df2['ENROLLMENT']*100\n",
        "\n",
        "df3= df2[['TERM_CODE','COURSE_SECTION', 'FT','ENROLLMENT', 'AVG_AGE', 'PCT_FRESHMEN','PCT_RACE_MINORITY','PCT_FEMALE','PCT_PELLSTATUS','PCT_POOR_PERFORMANCE']]\n",
        "\n",
        "df=df2[['FT', 'ENROLLMENT','PCT_RACE_MINORITY','PCT_POOR_PERFORMANCE']]"
      ],
      "metadata": {
        "id": "F5d74VTqyNQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-uyn-gvFj0_"
      },
      "source": [
        "# Causal Discovery with causal-learn\n",
        "\n",
        "We use the causal-learn library to perform causal discovery on the Auto-MPG dataset. We use three methods for causal discovery here: PC, FCI and GES. These methods are widely used and do not take much time to run. Hence, these are ideal for an introduction to the topic. Causal-learn provides a comprehensive list of well-tested causal-discovery methods, and readers are welcome to explore.\n",
        "\n",
        "The documentation for the methods used are as follows:\n",
        "- PC [[link]](https://causal-learn.readthedocs.io/en/latest/search_methods_index/Constraint-based%20causal%20discovery%20methods/PC.html)\n",
        "- GES [[link]](https://causal-learn.readthedocs.io/en/latest/search_methods_index/Score-based%20causal%20discovery%20methods/GES.html)\n",
        "- LiNGAM [[link]](https://causal-learn.readthedocs.io/en/latest/search_methods_index/Causal%20discovery%20methods%20based%20on%20constrained%20functional%20causal%20models/lingam.html#ica-based-lingam)\n",
        "\n",
        "More methods could be found in the causal-learn documentation [[link]](https://causal-learn.readthedocs.io/en/latest/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehqs9xC0Fj1C"
      },
      "source": [
        "Well, these two results are different, which is not rare when applying causal discovery on real-world dataset, since the required assumptions on the data-generating process are hard to verify.\n",
        "\n",
        "In addition, the graphs returned by PC and GES are CPDAGs instead of DAGs, so it is possible to have undirected edges (e.g., the result returned by GES). Thus, causal effect estimataion is difficult for those methods, since there may be absence of backdoor, instrumental or frontdoor variables. In order to get a DAG, we decide to try LiNGAM on our dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from causallearn.search.FCMBased import lingam\n",
        "model = lingam.ICALiNGAM()\n",
        "model.fit(df)\n",
        "\n",
        "from causallearn.search.FCMBased.lingam.utils import make_dot\n",
        "labels = [f'{col}' for i, col in enumerate(df.columns)]\n",
        "make_dot(model.adjacency_matrix_, labels=labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "GQ9gKXQDfkci",
        "outputId": "099f6cb0-f59f-40c4-e906-ab60e14c32ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_fastica.py:542: FutureWarning: Starting in v1.3, whiten='unit-variance' will be used by default.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"301pt\" height=\"305pt\"\n viewBox=\"0.00 0.00 301.48 305.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 301)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-301 297.48,-301 297.48,4 -4,4\"/>\n<!-- FT -->\n<g id=\"node1\" class=\"node\">\n<title>FT</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"185.34\" cy=\"-279\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"185.34\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">FT</text>\n</g>\n<!-- ENROLLMENT -->\n<g id=\"node2\" class=\"node\">\n<title>ENROLLMENT</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"127.34\" cy=\"-192\" rx=\"70.69\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"127.34\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">ENROLLMENT</text>\n</g>\n<!-- FT&#45;&gt;ENROLLMENT -->\n<g id=\"edge1\" class=\"edge\">\n<title>FT&#45;&gt;ENROLLMENT</title>\n<path fill=\"none\" stroke=\"black\" d=\"M174.7,-262.41C166.31,-250.11 154.42,-232.69 144.66,-218.38\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"147.43,-216.24 138.91,-209.95 141.65,-220.18 147.43,-216.24\"/>\n<text text-anchor=\"middle\" x=\"172.84\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">1.52</text>\n</g>\n<!-- PCT_POOR_PERFORMANCE -->\n<g id=\"node4\" class=\"node\">\n<title>PCT_POOR_PERFORMANCE</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"169.34\" cy=\"-18\" rx=\"124.28\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"169.34\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">PCT_POOR_PERFORMANCE</text>\n</g>\n<!-- FT&#45;&gt;PCT_POOR_PERFORMANCE -->\n<g id=\"edge3\" class=\"edge\">\n<title>FT&#45;&gt;PCT_POOR_PERFORMANCE</title>\n<path fill=\"none\" stroke=\"black\" d=\"M192.8,-261.55C206.93,-228.05 234.57,-149.74 215.34,-87 210.52,-71.28 200.7,-55.91 191.37,-43.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"194.05,-41.53 185.05,-35.93 188.6,-45.92 194.05,-41.53\"/>\n<text text-anchor=\"middle\" x=\"234.84\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">0.68</text>\n</g>\n<!-- PCT_RACE_MINORITY -->\n<g id=\"node3\" class=\"node\">\n<title>PCT_RACE_MINORITY</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"103.34\" cy=\"-105\" rx=\"103.18\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"103.34\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">PCT_RACE_MINORITY</text>\n</g>\n<!-- ENROLLMENT&#45;&gt;PCT_RACE_MINORITY -->\n<g id=\"edge2\" class=\"edge\">\n<title>ENROLLMENT&#45;&gt;PCT_RACE_MINORITY</title>\n<path fill=\"none\" stroke=\"black\" d=\"M122.48,-173.8C119.17,-162.05 114.7,-146.24 110.92,-132.84\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"114.28,-131.85 108.19,-123.18 107.54,-133.75 114.28,-131.85\"/>\n<text text-anchor=\"middle\" x=\"131.84\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">&#45;0.29</text>\n</g>\n<!-- PCT_RACE_MINORITY&#45;&gt;PCT_POOR_PERFORMANCE -->\n<g id=\"edge4\" class=\"edge\">\n<title>PCT_RACE_MINORITY&#45;&gt;PCT_POOR_PERFORMANCE</title>\n<path fill=\"none\" stroke=\"black\" d=\"M116.7,-86.8C126.18,-74.59 139.06,-57.99 149.71,-44.28\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"152.64,-46.22 156,-36.18 147.11,-41.93 152.64,-46.22\"/>\n<text text-anchor=\"middle\" x=\"152.84\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">0.13</text>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7fdefcb16500>"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsomCoUZFj1D"
      },
      "source": [
        "Now we have a DAG and are ready to estimate the causal effects based on that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTruHh9vFj1E"
      },
      "source": [
        "## Estimate causal effects using Linear Regression\n",
        "\n",
        "Now let us see the estimate of causal effect of *mpg* on *weight*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tygn3yY9Fj1E",
        "outputId": "c18e2ce2-6214-4b4b-9c55-db498b0afac5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:dowhy.causal_graph:Error: Pygraphviz cannot be loaded. No module named 'pygraphviz'\n",
            "Trying pydot ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimand type: EstimandType.NONPARAMETRIC_ATE\n",
            "\n",
            "### Estimand : 1\n",
            "Estimand name: backdoor\n",
            "Estimand expression:\n",
            "  d                           \n",
            "─────(E[PCT_POOR_PERFORMANCE])\n",
            "d[FT]                         \n",
            "Estimand assumption 1, Unconfoundedness: If U→{FT} and U→PCT_POOR_PERFORMANCE then P(PCT_POOR_PERFORMANCE|FT,,U) = P(PCT_POOR_PERFORMANCE|FT,)\n",
            "\n",
            "### Estimand : 2\n",
            "Estimand name: iv\n",
            "No such variable(s) found!\n",
            "\n",
            "### Estimand : 3\n",
            "Estimand name: frontdoor\n",
            "No such variable(s) found!\n",
            "\n",
            "Causal Estimate is 1.071344998100738\n"
          ]
        }
      ],
      "source": [
        "# Obtain valid dot format\n",
        "graph_dot = make_graph(model.adjacency_matrix_, labels=labels)\n",
        "\n",
        "# Define Causal Model\n",
        "model=CausalModel(\n",
        "        data = df,\n",
        "        treatment='FT',\n",
        "        outcome='PCT_POOR_PERFORMANCE',\n",
        "        graph=str_to_dot(graph_dot.source))\n",
        "\n",
        "# Identification\n",
        "identified_estimand = model.identify_effect(proceed_when_unidentifiable=True)\n",
        "print(identified_estimand)\n",
        "\n",
        "# Estimation\n",
        "estimate = model.estimate_effect(identified_estimand,\n",
        "                                method_name=\"backdoor.linear_regression\",\n",
        "                                control_value=0,\n",
        "                                treatment_value=1,\n",
        "                                confidence_intervals=True,\n",
        "                                test_significance=True)\n",
        "print(\"Causal Estimate is \" + str(estimate.value))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "refutation = model.refute_estimate(identified_estimand, estimate, method_name='placebo_treatment_refuter',\n",
        "                     placebo_type='permute', num_simulations=20)\n",
        "print(refutation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTMoymY2zl0W",
        "outputId": "59b8bb91-147f-4d28-a89c-ea9ce9432799"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:dowhy.causal_refuter:We assume a Normal Distribution as the sample has less than 100 examples.\n",
            "             Note: The underlying distribution may not be Normal. We assume that it approaches normal with the increase in sample size.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Refute: Use a Placebo Treatment\n",
            "Estimated effect:1.071344998100738\n",
            "New effect:-0.04208441911330594\n",
            "p value:0.4606182355292562\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "covariate_names = ['ENROLLMENT', 'PCT_RACE_MINORITY']  # Replace with actual covariate names\n",
        "\n",
        "# Iterate over covariates and compute CATE for each\n",
        "for covariate_name in covariate_names:\n",
        "    cate_results = model.estimate_effect(identified_estimand,\n",
        "                                        method_name=\"backdoor.linear_regression\",\n",
        "                                        control_value=0,\n",
        "                                        treatment_value=1,\n",
        "                                        confidence_intervals=True,\n",
        "                                        test_significance=True,\n",
        "                                        method_params={'subset_features': [covariate_name]})\n",
        "\n",
        "    # Print CATE value for each covariate\n",
        "    print(f\"CATE for {covariate_name} is {cate_results.value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujlCG90J-FkU",
        "outputId": "4438b5e5-486d-4d1c-e88d-a326159208f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CATE for ENROLLMENT is 1.071344998100738\n",
            "CATE for PCT_RACE_MINORITY is 1.071344998100738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "Yltt24C1CO3e",
        "outputId": "de5d206d-277c-4e50-d97e-5e903d252d70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:dowhy.causal_estimator:No common causes/confounders present. Propensity score based methods are not applicable\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "No common causes/confounders present. Propensity score based methods are not applicable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-169-4fb7ad39fedb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m estimate = model.estimate_effect(identified_estimand,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                  \u001b[0mmethod_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'backdoor.propensity_score_matching'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                  target_units='att')\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dowhy/causal_model.py\u001b[0m in \u001b[0;36mestimate_effect\u001b[0;34m(self, identified_estimand, method_name, control_value, treatment_value, test_significance, evaluate_effect_strength, confidence_intervals, target_units, effect_modifiers, fit_estimator, method_params)\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimator_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmethod_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcausal_estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m         return estimate_effect(\n\u001b[0m\u001b[1;32m    361\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_treatment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dowhy/causal_estimator.py\u001b[0m in \u001b[0;36mestimate_effect\u001b[0;34m(data, treatment, outcome, identifier_name, estimator, control_value, treatment_value, target_units, effect_modifiers, fit_estimator, method_params)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfit_estimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m         estimator.fit(\n\u001b[0m\u001b[1;32m    720\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m             \u001b[0meffect_modifier_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meffect_modifiers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dowhy/causal_estimators/propensity_score_matching_estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, data, effect_modifier_names)\u001b[0m\n\u001b[1;32m     95\u001b[0m                     \u001b[0mmethods\u001b[0m \u001b[0msupport\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mcurrently\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \"\"\"\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meffect_modifier_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meffect_modifier_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_estimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct_symbolic_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_target_estimand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dowhy/causal_estimators/propensity_score_estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, data, effect_modifier_names)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0merror_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"No common causes/confounders present. Propensity score based methods are not applicable\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# Check if the treatment is one-dimensional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: No common causes/confounders present. Propensity score based methods are not applicable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression, LassoCV\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from lightgbm import LGBMRegressor, LGBMClassifier\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "KKe-M6HR_KnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get estimate (T-Learner)\n",
        "estimate = model.estimate_effect(\n",
        "    identified_estimand=identified_estimand,\n",
        "    method_name='backdoor.econml.metalearners.TLearner',\n",
        "    target_units='ate',\n",
        "    method_params={\n",
        "        'init_params': {\n",
        "            'models': [\n",
        "                LGBMRegressor(n_estimators=200, max_depth=10),\n",
        "                LGBMRegressor(n_estimators=200, max_depth=10)\n",
        "            ]\n",
        "        },\n",
        "        'fit_params': {}\n",
        "    })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sI97zbAC_Zgm",
        "outputId": "f8f0e2a2-4b58-44b2-db3e-df7e4c33aae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/shap/utils/_clustering.py:35: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
            "  def _pt_shuffle_rec(i, indexes, index_mask, partition_tree, M, pos):\n",
            "/usr/local/lib/python3.10/dist-packages/shap/utils/_clustering.py:54: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
            "  def delta_minimization_order(all_masks, max_swap_size=100, num_passes=2):\n",
            "/usr/local/lib/python3.10/dist-packages/shap/utils/_clustering.py:63: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
            "  def _reverse_window(order, start, length):\n",
            "/usr/local/lib/python3.10/dist-packages/shap/utils/_clustering.py:69: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
            "  def _reverse_window_score_gain(masks, order, start, length):\n",
            "/usr/local/lib/python3.10/dist-packages/shap/utils/_clustering.py:77: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
            "  def _mask_delta_score(m1, m2):\n",
            "/usr/local/lib/python3.10/dist-packages/shap/links.py:5: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
            "  def identity(x):\n",
            "/usr/local/lib/python3.10/dist-packages/shap/links.py:10: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
            "  def _identity_inverse(x):\n",
            "/usr/local/lib/python3.10/dist-packages/shap/links.py:15: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
            "  def logit(x):\n",
            "/usr/local/lib/python3.10/dist-packages/shap/links.py:20: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
            "  def _logit_inverse(x):\n",
            "/usr/local/lib/python3.10/dist-packages/shap/utils/_masked_model.py:363: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
            "  def _build_fixed_single_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
            "/usr/local/lib/python3.10/dist-packages/shap/utils/_masked_model.py:385: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
            "  def _build_fixed_multi_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
            "/usr/local/lib/python3.10/dist-packages/shap/utils/_masked_model.py:428: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
            "  def _init_masks(cluster_matrix, M, indices_row_pos, indptr):\n",
            "/usr/local/lib/python3.10/dist-packages/shap/utils/_masked_model.py:439: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
            "  def _rec_fill_masks(cluster_matrix, indices_row_pos, indptr, indices, M, ind):\n",
            "/usr/local/lib/python3.10/dist-packages/shap/maskers/_tabular.py:186: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
            "  def _single_delta_mask(dind, masked_inputs, last_mask, data, x, noop_code):\n",
            "/usr/local/lib/python3.10/dist-packages/shap/maskers/_tabular.py:197: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
            "  def _delta_masking(masks, x, curr_delta_inds, varying_rows_out,\n",
            "/usr/local/lib/python3.10/dist-packages/shap/maskers/_image.py:175: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
            "  def _jit_build_partition_tree(xmin, xmax, ymin, ymax, zmin, zmax, total_ywidth, total_zwidth, M, clustering, q):\n",
            "/usr/local/lib/python3.10/dist-packages/shap/explainers/_partition.py:676: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
            "  def lower_credit(i, value, M, values, clustering):\n",
            "The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
            "The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Expected 2D array, got scalar array instead:\narray=None.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-167-d7762eb41e99>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get estimate (T-Learner)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m estimate = model.estimate_effect(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0midentified_estimand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midentified_estimand\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmethod_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'backdoor.econml.metalearners.TLearner'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtarget_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ate'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dowhy/causal_model.py\u001b[0m in \u001b[0;36mestimate_effect\u001b[0;34m(self, identified_estimand, method_name, control_value, treatment_value, test_significance, evaluate_effect_strength, confidence_intervals, target_units, effect_modifiers, fit_estimator, method_params)\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimator_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmethod_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcausal_estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m         return estimate_effect(\n\u001b[0m\u001b[1;32m    361\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_treatment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dowhy/causal_estimator.py\u001b[0m in \u001b[0;36mestimate_effect\u001b[0;34m(data, treatment, outcome, identifier_name, estimator, control_value, treatment_value, target_units, effect_modifiers, fit_estimator, method_params)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfit_estimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m         estimator.fit(\n\u001b[0m\u001b[1;32m    720\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m             \u001b[0meffect_modifier_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meffect_modifiers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dowhy/causal_estimators/econml.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, data, effect_modifier_names, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0marg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnamed_data_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnamed_data_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mestimator_named_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         }\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mestimator_data_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/econml/_cate_estimator.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, Y, T, inference, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0minference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;31m# call the wrapped fit method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_postfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minference\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/econml/metalearners/_metalearners.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, Y, T, X, inference)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# Check inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output_T\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcategories\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/econml/utilities.py\u001b[0m in \u001b[0;36mcheck_inputs\u001b[0;34m(Y, T, X, W, multi_output_T, multi_output_Y)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \"\"\"\n\u001b[0;32m--> 512\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_output_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_output_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mW\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         )\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1107\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    892\u001b[0m             \u001b[0;31m# If input is scalar raise error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    895\u001b[0m                     \u001b[0;34m\"Expected 2D array, got scalar array instead:\\narray={}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got scalar array instead:\narray=None.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaCBROboBRtn",
        "outputId": "8d59e11c-fbef-401d-ea0b-a2e1f2cb9836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<dowhy.causal_identifier.identified_estimand.IdentifiedEstimand at 0x7fdef832cd00>"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    },
    "metadata": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}